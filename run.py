# -*- coding: utf-8 -*-
import os
import sys
import pickle
import numpy as np
import tensorflow as tf
import arabic_reshaper
from bidi.algorithm import get_display
from tensorflow.keras.models import load_model

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
MODEL_NAME = "ML-T1-Advanced"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"
MAX_SEQUENCE_LEN = 48

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ Tokenizer
def load_model_and_tokenizer():
    print("ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    model = load_model(MODEL_FILE)
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
    
    print("ğŸ”  Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Tokenizer...")
    with open(TOKENIZER_FILE, 'rb') as f:
        tokenizer = pickle.load(f)
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Tokenizer Ø¨Ù†Ø¬Ø§Ø­")
    
    return model, tokenizer

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
def generate_text(seed_text, next_words, model, tokenizer, temperature=0.7):
    output = seed_text
    
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([output])[0]
        token_list = [t for t in token_list if t < tokenizer.num_words] or [1]
        token_list = token_list[-MAX_SEQUENCE_LEN:]
        token_list = tf.keras.preprocessing.sequence.pad_sequences(
            [token_list], maxlen=MAX_SEQUENCE_LEN, padding='pre'
        )
        
        predictions = model.predict(token_list, verbose=0)[0]
        
        # ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        predictions = np.log(predictions + 1e-10) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        output += " " + predicted_word
        
        if predicted_word in [".", "ØŸ", "!"]:
            break
            
    # ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ù„Ø¹Ø±Ø¶
    reshaped_text = arabic_reshaper.reshape(output)
    return get_display(reshaped_text)

# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
def interactive_interface(model, tokenizer):
    print("\n" + "=" * 70)
    print("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ ML-T1ØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    print("ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ØŒ Ø£Ùˆ ÙƒØªØ§Ø¨Ø© 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("=" * 70)
    
    while True:
        try:
            user_input = input("\n> ").strip()
            
            if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
                print("âœ¨ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! ÙƒØ§Ù† Ø­Ø¯ÙŠØ«Ø§Ù‹ Ù…Ù…ØªØ¹Ø§Ù‹.")
                break
                
            if not user_input:
                print("â“ ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ù„Ù… ØªØ¯Ø®Ù„ Ø£ÙŠ Ù†ØµØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ")
                continue
                
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            response = generate_text(user_input, 30, model, tokenizer, temperature=0.7)
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯
            print(f"\nğŸ’¡ ML-T1:\n{response}")
            
        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
            print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„...")

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model, tokenizer = load_model_and_tokenizer()
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    interactive_interface(model, tokenizer)
