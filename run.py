# -*- coding: utf-8 -*-
import os
import sys
import pickle
import numpy as np
import tensorflow as tf
import arabic_reshaper
from bidi.algorithm import get_display
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LayerNormalization, Dense, Dropout, MultiHeadAttention, GlobalAveragePooling1D

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ØªØ¹Ø·ÙŠÙ„ Ù…Ø¹Ø¸Ù… Ø³Ø¬Ù„Ø§Øª TensorFlow
os.environ['OMP_NUM_THREADS'] = '4'  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆÙŠØ©
tf.get_logger().setLevel('ERROR')  # ØªØ¹Ø·ÙŠÙ„ ØªØ­Ø°ÙŠØ±Ø§Øª TensorFlow

# Ø¶Ø¨Ø· Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø£Ù†ÙˆÙŠØ© Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©
tf.config.threading.set_intra_op_parallelism_threads(4)
tf.config.threading.set_inter_op_parallelism_threads(4)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
MODEL_NAME = "ML-T1-Advanced"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"
MAX_SEQUENCE_LEN = 48
EMBEDDING_DIM = 128
VOCAB_SIZE = 30000  # ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡ Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠ

# ØªØ¹Ø±ÙŠÙ ÙØ¦Ø© PositionalEncoding Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, maxlen, embed_dim, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.maxlen = maxlen
        self.embed_dim = embed_dim
        
    def build(self, input_shape):
        super().build(input_shape)
        position = np.arange(self.maxlen)[:, np.newaxis]
        i = np.arange(self.embed_dim)[np.newaxis, :]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(self.embed_dim))
        angle_rads = position * angle_rates
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø¯ÙŠØ©
        sines = np.sin(angle_rads[:, 0::2])
        cosines = np.cos(angle_rads[:, 1::2])
        self.pos_encoding = np.concatenate([sines, cosines], axis=-1)
        self.pos_encoding = self.pos_encoding[np.newaxis, ...]
        self.pos_encoding = tf.constant(self.pos_encoding, dtype=tf.float32)
        
    def call(self, inputs):
        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]
    
    def compute_output_shape(self, input_shape):
        return input_shape

# Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø³Ù† Ù„Ù„Ø£Ø¯Ø§Ø¡
def build_optimized_model():
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
    inputs = Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    embedding = Embedding(
        input_dim=VOCAB_SIZE, 
        output_dim=EMBEDDING_DIM,
        name='embedding'
    )(inputs)
    
    # Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ
    positional_encoding = PositionalEncoding(MAX_SEQUENCE_LEN, EMBEDDING_DIM, name='pos_encoding')(embedding)
    x = Dropout(0.3)(positional_encoding)
    
    # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
    for i in range(3):  # 3 Ø·Ø¨Ù‚Ø§Øª transformer (Ø®ÙÙŠÙØ©)
        # Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        attn_output = MultiHeadAttention(
            num_heads=4,
            key_dim=EMBEDDING_DIM // 4,  # ØªÙ‚Ø³ÙŠÙ… Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø±Ø£Ø³
            name=f'mha_{i}'
        )(x, x)
        
        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
        x = LayerNormalization(epsilon=1e-6, name=f'ln1_{i}')(x + attn_output)
        
        # Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        ffn = Dense(EMBEDDING_DIM * 2, activation='gelu', name=f'ffn1_{i}')(x)
        ffn = Dense(EMBEDDING_DIM, name=f'ffn2_{i}')(ffn)
        x = LayerNormalization(epsilon=1e-6, name=f'ln2_{i}')(x + ffn)
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    x = GlobalAveragePooling1D(name='gap')(x)
    x = Dense(256, activation='gelu', name='dense1')(x)
    outputs = Dense(VOCAB_SIZE, activation='softmax', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ Tokenizer
def load_model_and_tokenizer():
    print("ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹
    model = build_optimized_model()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    try:
        model.load_weights(MODEL_FILE)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {str(e)}")
        sys.exit(1)
    
    print("ğŸ”  Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Tokenizer...")
    try:
        with open(TOKENIZER_FILE, 'rb') as f:
            tokenizer = pickle.load(f)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Tokenizer Ø¨Ù†Ø¬Ø§Ø­")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Tokenizer: {e}")
        sys.exit(1)

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø¨ÙƒÙØ§Ø¡Ø©
def generate_text(seed_text, next_words, model, tokenizer, temperature=0.7):
    if not seed_text:
        return ""
    
    output = seed_text
    token_sequence = []
    
    for _ in range(next_words):
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„ Ø±Ù…ÙˆØ²
        tokens = tokenizer.texts_to_sequences([output])[0]
        tokens = [t for t in tokens if t < tokenizer.num_words] or [1]
        token_sequence = tokens[-MAX_SEQUENCE_LEN:]
        
        # Ø­Ø´Ùˆ Ø§Ù„ØªØ³Ù„Ø³Ù„
        padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(
            [token_sequence], maxlen=MAX_SEQUENCE_LEN, padding='pre'
        )
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        predictions = model.predict(padded_sequence, verbose=0)[0]
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ØªÙˆØ²ÙŠØ¹
        predictions = np.log(predictions + 1e-10) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        output += " " + predicted_word
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø°ÙƒÙŠ Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        if predicted_word in [".", "ØŸ", "!"]:
            break
            
    # ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ù„Ø¹Ø±Ø¶
    reshaped_text = arabic_reshaper.reshape(output)
    return get_display(reshaped_text)

# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
def interactive_interface(model, tokenizer):
    print("\n" + "=" * 70)
    print("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ ML-T1ØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    print("Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…: 4 Ù†ÙˆÙ‰ | 16GB RAM | 100GB ØªØ®Ø²ÙŠÙ†")
    print("ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ØŒ Ø£Ùˆ ÙƒØªØ§Ø¨Ø© 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("=" * 70)
    
    while True:
        try:
            user_input = input("\n> ").strip()
            
            if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
                print("âœ¨ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! ÙƒØ§Ù† Ø­Ø¯ÙŠØ«Ø§Ù‹ Ù…Ù…ØªØ¹Ø§Ù‹.")
                break
                
            if not user_input:
                print("â“ ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ù„Ù… ØªØ¯Ø®Ù„ Ø£ÙŠ Ù†ØµØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ")
                continue
                
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            response = generate_text(user_input, 30, model, tokenizer, temperature=0.7)
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯
            print(f"\nğŸ’¡ ML-T1:\n{response}")
            
        except KeyboardInterrupt:
            print("\nâœ¨ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! ÙƒØ§Ù† Ø­Ø¯ÙŠØ«Ø§Ù‹ Ù…Ù…ØªØ¹Ø§Ù‹.")
            break
        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
            print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
if __name__ == "__main__":
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model, tokenizer = load_model_and_tokenizer()
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    interactive_interface(model, tokenizer)
