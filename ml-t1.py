# -*- coding: utf-8 -*-
"""
Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø±Ø¨ÙŠ Ø°Ø§ØªÙŠ Ø§Ù„ØªØ¹Ù„Ù…
"""

import os
import re
import pickle
import numpy as np
import requests
from bs4 import BeautifulSoup
from datasets import load_dataset
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (
    Input, Embedding, LayerNormalization, Dense,
    Dropout, MultiHeadAttention, GlobalAveragePooling1D,
    Bidirectional, LSTM, Attention
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, 
    ReduceLROnPlateau, TensorBoard
)
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
import arabic_reshaper
from bidi.algorithm import get_display
import gc
import datetime
import json

# ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    print(f"âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ ØªØ³Ø±ÙŠØ¹ GPU: {len(gpus)} ÙˆØ­Ø¯Ø©")
else:
    print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GPUØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ CPU")

tf.config.optimizer.set_jit(True)
tf.config.run_functions_eagerly(True)

os.environ['OMP_NUM_THREADS'] = '4'  # Ù„Ø§Ø³ØªØºÙ„Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆÙŠØ©
os.environ['TF_NUM_INTRAOP_THREADS'] = '4'
os.environ['TF_NUM_INTEROP_THREADS'] = '4'

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
MODEL_NAME = "ML-T1-Advanced"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"
KNOWLEDGE_FILE = f"{MODEL_NAME}_knowledge.json"
LOG_DIR = f"{MODEL_NAME}_logs"

# Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒÙŠ
FF_DIM = 384
NUM_LAYERS = 3
VOCAB_LIMIT = 25000
CONTEXT_SIZE = 3  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„Ø³ÙŠØ§Ù‚ 

MAX_SEQUENCE_LEN = 48  # ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 64
EMBEDDING_DIM = 128    # ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 192
NUM_HEADS = 4          # ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 6
FF_DIM = 256           # ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 384
BATCH_SIZE = 16        # ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 32


# 2. Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠØ©
TRAINING_STAGES = [
    {"name": "Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ£Ø³ÙŠØ³ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ", "sample_size": 5000, "epochs": 8},
    {"name": "Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ", "sample_size": 20000, "epochs": 12},
    {"name": "Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", "sample_size": 50000, "epochs": 15},
    {"name": "Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ø§Ø¨ØªÙƒØ§Ø±", "sample_size": 100000, "epochs": 18},
    {"name": "Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ…ÙŠØ² ÙˆØ§Ù„Ù‚ÙŠØ§Ø¯Ø©", "sample_size": 200000, "epochs": 20}
]

class IntelligentArabicAI:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.knowledge_base = self.load_knowledge()
        self.session_context = []
        
    def load_knowledge(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø°Ø§ØªÙŠØ©"""
        try:
            if os.path.exists(KNOWLEDGE_FILE):
                with open(KNOWLEDGE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: {e}")
        return {
            "facts": {},
            "qa_pairs": {},
            "last_updated": str(datetime.datetime.now())
        }
    
    def save_knowledge(self):
        """Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø°Ø§ØªÙŠØ©"""
        try:
            self.knowledge_base["last_updated"] = str(datetime.datetime.now())
            with open(KNOWLEDGE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
            print("ğŸ’¾ ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: {e}")
    
    def search_online(self, query):
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª"""
        try:
            print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: '{query}'")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            url = f"https://www.google.com/search?q={query}&hl=ar"
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            results = []
            for g in soup.find_all('div', class_='tF2Cxc'):
                title = g.find('h3').text if g.find('h3') else ""
                snippet = g.find('div', class_='VwiC3b').text if g.find('div', class_='VwiC3b') else ""
                if title and snippet:
                    results.append({"title": title, "content": snippet})
            
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
            if results:
                self.knowledge_base["facts"][query] = results[:3]  # Ø­ÙØ¸ Ø£ÙØ¶Ù„ 3 Ù†ØªØ§Ø¦Ø¬
                self.save_knowledge()
            
            return results
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []

    def update_context(self, text):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«ÙŠ"""
        self.session_context.append(text)
        if len(self.session_context) > CONTEXT_SIZE:
            self.session_context.pop(0)

    def get_context(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        return " ".join(self.session_context)
    
    def understand_query(self, query):
        """ÙÙ‡Ù… Ù†ÙŠØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        intent_keywords = {
            "ØªØ¹Ø±ÙŠÙ": ["Ù…Ø§ Ù‡Ùˆ", "Ù…Ù† Ù‡Ùˆ", "Ù…Ø§ Ù‡ÙŠ", "ØªØ¹Ø±ÙŠÙ", "Ù…ÙÙ‡ÙˆÙ…"],
            "Ù…Ù‚Ø§Ø±Ù†Ø©": ["Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†", "Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ†", "Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙØ¶Ù„"],
            "ØªÙØ³ÙŠØ±": ["ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ø´Ø±Ø­", "ØªÙØ³ÙŠØ±", "Ø³Ø¨Ø¨"],
            "Ø¨Ø­Ø«": ["Ø§Ø¨Ø­Ø« Ø¹Ù†", "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†", "Ø£Ø±ØºØ¨ ÙÙŠ Ù…Ø¹Ø±ÙØ©", "Ø¨Ø­Ø« Ø¹Ù†"],
            "Ø±Ø£ÙŠ": ["Ø±Ø£ÙŠÙƒ", "Ù…Ø§ ØªØ¸Ù†", "Ù…Ø§ Ø±Ø£ÙŠÙƒ", "ØªÙˆØµÙŠØ©", "Ù†ØµÙŠØ­Ø©"]
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙŠØ©
        intent = "Ø¹Ø§Ù…"
        for key, words in intent_keywords.items():
            if any(word in query for word in words):
                intent = key
                break
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
        topic = query
        for word in query.split():
            if word in ["Ø¹Ù†", "ÙÙŠ", "Ø¨Ø®ØµÙˆØµ", "Ø­ÙˆÙ„"]:
                idx = query.index(word)
                topic = query[idx+len(word):].strip()
                break
        
        return {"intent": intent, "topic": topic}
    
    def generate_response(self, query, temperature=0.7):
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø°ÙƒÙŠ Ù…Ø¹ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
        # ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„
        analysis = self.understand_query(query)
        intent = analysis["intent"]
        topic = analysis["topic"]
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ù‚
        self.update_context(query)
        context = self.get_context()
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø°Ø§ØªÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
        if topic in self.knowledge_base["facts"]:
            knowledge = self.knowledge_base["facts"][topic][0]["content"]
            prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ù…Ø¹Ø±ÙØ©: {knowledge}\nØ§Ù„Ø³ÙŠØ§Ù‚: {context}\nØ§Ù„Ø±Ø¯:"
        else:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if intent in ["ØªØ¹Ø±ÙŠÙ", "Ø¨Ø­Ø«"]:
                search_results = self.search_online(topic)
                if search_results:
                    knowledge = search_results[0]["content"]
                    prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ù…Ø¹Ø±ÙØ©: {knowledge}\nØ§Ù„Ø³ÙŠØ§Ù‚: {context}\nØ§Ù„Ø±Ø¯:"
                else:
                    prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø³ÙŠØ§Ù‚: {context}\nØ§Ù„Ø±Ø¯:"
            else:
                prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø³ÙŠØ§Ù‚: {context}\nØ§Ù„Ø±Ø¯:"
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        response = self.generate_text(prompt, 25, temperature)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø¯ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        if intent == "Ø±Ø£ÙŠ":
            response = "Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ØŒ Ø£Ø±Ù‰ Ø£Ù† " + response
        elif intent == "Ù…Ù‚Ø§Ø±Ù†Ø©":
            response = "Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ø¬Ø¯ Ø£Ù† " + response
        elif intent == "ØªÙØ³ÙŠØ±":
            response = "Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ Ù‡Ùˆ Ø£Ù† " + response
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø§Ù„Ø±Ø¯
        self.update_context(response)
        
        return response

# 3. ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_and_preprocess_data():
    """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©"""
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©...")
    sources = [
        "wiki40b/ar",
        "arabic_billion_words",
        "oscar-arabic"
    ]
    
    all_texts = []
    for source in sources:
        try:
            print(f"ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ {source}...")
            dataset = load_dataset(source, split="train", streaming=True)
            for item in dataset.take(20000):
                if 'text' in item:
                    all_texts.append(item['text'])
                elif 'content' in item:
                    all_texts.append(item['content'])
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(all_texts)} Ù†ØµÙ‹Ø§ Ù…Ù† {source}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {source}: {e}")
    
    if not all_texts:
        print("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©")
        return generate_sample_data(20000)
    
    return all_texts

def generate_sample_data(num_samples):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© ØºÙ†ÙŠØ©"""
    base_texts = [
        "Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…ØŒ ÙØ§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù„Ù… Ø¯Ø§Ø¦Ù…Ø§Ù‹",
        "Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„ØµØºØ± ÙƒØ§Ù„Ù†Ù‚Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ø±ØŒ Ù„Ø°Ù„Ùƒ ÙŠØ¬Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø¨ÙƒØ±",
        "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø§Ù„Ø¶Ø§Ø¯ØŒ ÙˆÙ‡ÙŠ Ù…Ù† Ø£Ù‚Ø¯Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø³Ø§Ù…ÙŠØ©",
        "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ØªØ³Ø§Ù‡Ù… ÙÙŠ ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª ÙˆØ§Ø²Ø¯Ù‡Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ",
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ…Ø«Ù„ Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©ØŒ ÙˆØ³ÙŠØºÙŠØ± ÙˆØ¬Ù‡ Ø§Ù„Ø¹Ø§Ù„Ù… ÙÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©",
        "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ø¹ Ø¢ÙØ§Ù‚ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† ÙˆØªØ²ÙŠØ¯ Ù…Ù† Ù…Ø¹Ø±ÙØªÙ‡ ÙˆØªØ·ÙˆØ± Ù…Ù‡Ø§Ø±Ø§ØªÙ‡ Ø§Ù„ÙÙƒØ±ÙŠØ©",
        "Ø§Ù„ØªØ§Ø±ÙŠØ® ÙŠØ¹Ù„Ù…Ù†Ø§ Ø¯Ø±ÙˆØ³Ø§Ù‹ Ù‚ÙŠÙ…Ø© Ø¹Ù† ØµØ¹ÙˆØ¯ ÙˆØ³Ù‚ÙˆØ· Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø£Ù…Ù…",
        "Ø§Ù„ÙÙ„Ø³ÙØ© ØªØ³Ø§Ø¹Ø¯Ù†Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙˆØ¬ÙˆØ¯ÙŠØ© Ø§Ù„ÙƒØ¨Ø±Ù‰",
        "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù‡ÙŠ Ù„ØºØ© Ø§Ù„ÙƒÙˆÙ†ØŒ ÙˆÙ‡ÙŠ Ø£Ø³Ø§Ø³ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ÙŠØ©",
        "Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ø§Ø¨ØªÙƒØ§Ø± Ù‡Ù…Ø§ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ø¨Ø´Ø±ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª"
    ]
    
    # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
    texts = []
    for i in range(num_samples):
        base = base_texts[i % len(base_texts)]
        variations = [
            f"ÙÙŠ Ù…Ø¬Ø§Ù„ {base.split()[0]}ØŒ {base}",
            f"Ù…Ù† Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø£Ù† {base}",
            f"ØªØ¹ØªØ¨Ø± {base}",
            f"Ø¨Ù„Ø§ Ø´ÙƒØŒ {base}",
            f"ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø­Ø¯ÙŠØ«ØŒ {base}",
            f"Ø£Ø«Ø¨ØªØª Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø£Ù† {base}",
            f"Ù…Ù† ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± Ø¹Ù„Ù…ÙŠØ©ØŒ {base}",
            f"ÙŠØ¹ØªÙ‚Ø¯ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø£Ù† {base}"
        ]
        texts.append(variations[i % len(variations)])
    return texts

def preprocess_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    if not isinstance(text, str):
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
    text = re.sub(r'[\u064B-\u065F\u0670]', '', text)
    text = re.sub(r'[^\u0600-\u06FF0-9\sØŒØ›:.,ØŸ!()\-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    if len(text.split()) > 80:
        parts = re.split(r'[.ØŒØ›ØŸ!]', text)
        text = ' '.join(parts[:4])
    
    return text

# 4. ØªØ¬Ù‡ÙŠØ² Tokenizer
def prepare_tokenizer(sentences):
    """Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ù…ØªÙ‚Ø¯Ù…"""
    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ø°ÙƒÙŠ...")
    tokenizer = Tokenizer(
        num_words=VOCAB_LIMIT, 
        oov_token="<OOV>",
        filters='',
        lower=False
    )
    tokenizer.fit_on_texts(sentences)
    
    # Ø­ÙØ¸ Tokenizer
    with open(TOKENIZER_FILE, "wb") as f:
        pickle.dump(tokenizer, f)
    
    vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
    print(f"ğŸ”¤ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø°ÙƒÙŠ: {vocab_size}")
    return tokenizer, vocab_size

# 5. Ø·Ø¨Ù‚Ø§Øª Transformer Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
class ContextAwareTransformer(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(ContextAwareTransformer, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="gelu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
        self.context_aware = Bidirectional(LSTM(embed_dim//2, return_sequences=True))

    def call(self, inputs, training=False):
        # Ø¯Ù…Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚
        contextual = self.context_aware(inputs)
        
        attn_output = self.att(contextual, contextual)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, maxlen, embed_dim):
        super(PositionalEncoding, self).__init__()
        self.pos_encoding = self.positional_encoding(maxlen, embed_dim)

    def get_angles(self, position, i, embed_dim):
        angles = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(embed_dim, tf.float32))
        return position * angles

    def positional_encoding(self, maxlen, embed_dim):
        angle_rads = self.get_angles(
            position=tf.range(maxlen, dtype=tf.float32)[:, tf.newaxis],
            i=tf.range(embed_dim, dtype=tf.float32)[tf.newaxis, :],
            embed_dim=embed_dim
        )
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø¯ÙŠØ©
        sines = tf.math.sin(angle_rads[:, 0::2])
        cosines = tf.math.cos(angle_rads[:, 1::2])
        
        pos_encoding = tf.concat([sines, cosines], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, ...]
        return tf.cast(pos_encoding, tf.float32)

    def call(self, inputs):
        seq_len = tf.shape(inputs)[1]
        return inputs + self.pos_encoding[:, :seq_len, :]

# 6. Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ
def build_intelligent_model(vocab_size):
    """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
    inputs = Input(shape=(MAX_SEQUENCE_LEN,))
    
    # Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    embedding = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM)(inputs)
    
    # Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ
    positional_encoding = PositionalEncoding(MAX_SEQUENCE_LEN, EMBEDDING_DIM)(embedding)
    x = Dropout(0.3)(positional_encoding)
    
    # Ø·Ø¨Ù‚Ø§Øª Transformer Ù…Ø¹ Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ
    for _ in range(NUM_LAYERS):
        transformer_block = ContextAwareTransformer(EMBEDDING_DIM, NUM_HEADS, FF_DIM)
        x = transformer_block(x, training=False)
    
    # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    context_vector = Attention()([x, x])
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚
    x = GlobalAveragePooling1D()(context_vector)
    x = Dropout(0.2)(x)
    
    # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚
    x = Dense(FF_DIM, activation="gelu", kernel_regularizer=l2(0.001))(x)
    x = LayerNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(FF_DIM // 2, activation="gelu", kernel_regularizer=l2(0.001))(x)
    x = LayerNormalization()(x)
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    outputs = Dense(vocab_size, activation="softmax")(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    
    # Ø§Ù„Ù…ØªØ±Ø¬Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    optimizer = Adam(
        learning_rate=0.0001,
        beta_1=0.9,
        beta_2=0.98,
        epsilon=1e-9,
        clipnorm=1.0
    )
    
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )
    
    model.summary()
    return model

# 7. Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
class IntelligentDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, sentences, tokenizer, batch_size, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.sentences) / self.batch_size))
    
    def __getitem__(self, index):
        batch_sentences = self.sentences[index*self.batch_size:(index+1)*self.batch_size]
        X, y = [], []
        
        for sentence in batch_sentences:
            if not sentence:
                continue
                
            tokens = self.tokenizer.texts_to_sequences([sentence])[0]
            tokens = [t if t < self.vocab_size else 1 for t in tokens]
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø§Øª ÙØ¹Ø§Ù„Ø©
            if len(tokens) > 1:
                # Ø£Ø®Ø° Ø¬Ø²Ø¡ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø©
                if len(tokens) > MAX_SEQUENCE_LEN + 1:
                    start_idx = np.random.randint(0, len(tokens) - MAX_SEQUENCE_LEN - 1)
                    seq = tokens[start_idx:start_idx + MAX_SEQUENCE_LEN + 1]
                else:
                    seq = tokens[:MAX_SEQUENCE_LEN + 1]
                
                # Ø­Ø´Ùˆ Ø§Ù„ØªØ³Ù„Ø³Ù„
                padded_seq = pad_sequences([seq], maxlen=MAX_SEQUENCE_LEN + 1, padding='pre')[0]
                X.append(padded_seq[:-1])
                y.append(padded_seq[-1])
        
        if len(X) == 0:
            return np.zeros((1, MAX_SEQUENCE_LEN)), np.zeros((1,))
        
        return np.array(X), np.array(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.sentences)

# 8. Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠ
def generate_intelligent_text(seed_text, next_words, model, tokenizer, temperature=0.7):
    """ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø°ÙƒÙŠ Ù…Ø¹ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
    if not seed_text:
        return ""
    
    output = seed_text
    
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([output])[0]
        token_list = [t if t < VOCAB_LIMIT else 1 for t in token_list]
        
        if not token_list:
            token_list = [1]
            
        token_list = token_list[-MAX_SEQUENCE_LEN:]
        token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LEN, padding='pre')
        
        predictions = model.predict(token_list, verbose=0)[0]
        
        # ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        predictions = np.log(predictions + 1e-10) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø°ÙƒÙŠ
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        output += " " + predicted_word
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©
        if predicted_word in [".", "ØŸ", "!"] and np.random.random() > 0.25:
            break
        if len(output.split()) > next_words * 1.5:
            break
            
    # ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ù„Ø¹Ø±Ø¶
    reshaped_text = arabic_reshaper.reshape(output)
    bidi_text = get_display(reshaped_text)
    return bidi_text

# 9. Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°Ø§ØªÙŠ
def self_learning_loop(ai, texts):
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
    print("\nğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ...")
    
    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    sentences = []
    for text in texts:
        cleaned = preprocess_text(text)
        if cleaned and len(cleaned.split()) > 4:
            sentences.append(cleaned)
    
    print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©: {len(sentences)}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer
    tokenizer, vocab_size = prepare_tokenizer(sentences)
    ai.tokenizer = tokenizer
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    ai.model = build_intelligent_model(vocab_size)
    
    # Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    callbacks = [
        ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_accuracy', mode='max'),
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        TensorBoard(log_dir=LOG_DIR)
    ]
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø­Ù„ Ù…ØªØ¹Ø¯Ø¯Ø©
    for stage in TRAINING_STAGES:
        print(f"\n{'='*70}")
        print(f"ğŸ§  {stage['name']}")
        print(f"ğŸ“ˆ Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø©: {stage['sample_size']} Ø¬Ù…Ù„Ø©")
        print(f"â±ï¸ Ø§Ù„Ø¹ØµÙˆØ±: {stage['epochs']}")
        print(f"{'='*70}")
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        sample_size = min(stage['sample_size'], len(sentences))
        stage_sentences = np.random.choice(sentences, size=sample_size, replace=False)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        train_sents, val_sents = train_test_split(stage_sentences, test_size=0.1, random_state=42)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        train_gen = IntelligentDataGenerator(train_sents, tokenizer, BATCH_SIZE)
        val_gen = IntelligentDataGenerator(val_sents, tokenizer, BATCH_SIZE, shuffle=False)
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        history = ai.model.fit(
            train_gen,
            epochs=stage['epochs'],
            validation_data=val_gen,
            callbacks=callbacks,
            verbose=1
        )
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        print("\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ:")
        test_queries = [
            "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
            "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø£Ù† ÙŠÙÙŠØ¯ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŸ",
            "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŸ",
            "ÙƒÙŠÙ Ø£Ø¨Ø¯Ø£ ÙÙŠ ØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"
        ]
        
        for query in test_queries:
            response = ai.generate_response(query)
            reshaped_query = arabic_reshaper.reshape(query)
            print(f"\nâ“ Ø§Ù„Ø³Ø¤Ø§Ù„: {get_display(reshaped_query)}")
            print(f"ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©:\n{response}")
            print("â”€" * 70)
    
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
    return ai

# 10. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©
def intelligent_interface(ai):
    """ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ø°ÙƒÙŠØ©"""
    print("\n" + "=" * 70)
    print("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ ML-T1ØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    print("ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ØŒ Ø£Ùˆ ÙƒØªØ§Ø¨Ø© 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("=" * 70)
    
    while True:
        try:
            user_input = input("\n> ").strip()
            
            if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
                print("âœ¨ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! ÙƒØ§Ù† Ø­Ø¯ÙŠØ«Ø§Ù‹ Ù…Ù…ØªØ¹Ø§Ù‹.")
                break
                
            if not user_input:
                print("â“ ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ù„Ù… ØªØ¯Ø®Ù„ Ø£ÙŠ Ù†ØµØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ")
                continue
                
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø®Ø§ØµØ©
            if user_input.startswith("!ØªØ¹Ù„Ù…"):
                topic = user_input[5:].strip()
                if topic:
                    print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØªØ¹Ù„Ù… Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù†: {topic}")
                    ai.search_online(topic)
                    print("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­")
                else:
                    print("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶ÙˆØ¹ Ù„Ù„ØªØ¹Ù„Ù…")
                continue
                
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ
            response = ai.generate_response(user_input)
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø¬Ù…ÙŠÙ„
            reshaped_response = arabic_reshaper.reshape(response)
            bidi_response = get_display(reshaped_response)
            print(f"\nğŸ’¡ ML-T1:\n{bidi_response}")
            
        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
            print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„...")
            ai.session_context = []

# 11. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    ai = IntelligentArabicAI()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    texts = load_and_preprocess_data()
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°Ø§ØªÙŠ
    if not os.path.exists(MODEL_FILE):
        ai = self_learning_loop(ai, texts)
    else:
        print("ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹...")
        ai.model = load_model(MODEL_FILE, custom_objects={
            'ContextAwareTransformer': ContextAwareTransformer,
            'PositionalEncoding': PositionalEncoding
        })
        with open(TOKENIZER_FILE, 'rb') as f:
            ai.tokenizer = pickle.load(f)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    intelligent_interface(ai)

if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    os.environ['OMP_NUM_THREADS'] = str(os.cpu_count() or 4)
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    main()
