# -*- coding: utf-8 -*-
"""
Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø¨Ù†ÙŠØ© Transformer Ù…ØªÙ‚Ø¯Ù…Ø©
"""

import os
import re
import pickle
import numpy as np
from datasets import load_dataset
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (
    Input, Embedding, LayerNormalization, Dense,
    Dropout, MultiHeadAttention, GlobalAveragePooling1D,
    Bidirectional, LSTM
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, 
    ReduceLROnPlateau, TensorBoard
)
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
import arabic_reshaper
from bidi.algorithm import get_display
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import tensorflow_text as tf_text

# ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
tf.config.optimizer.set_jit(True)
tf.config.run_functions_eagerly(True)

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
MODEL_NAME = "ML-T1-Transformer"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"
LOG_DIR = f"{MODEL_NAME}_logs"

# Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
MAX_SEQUENCE_LEN = 64  # Ø²ÙŠØ§Ø¯Ø© Ø·ÙˆÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
EMBEDDING_DIM = 512    # Ø²ÙŠØ§Ø¯Ø© Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
NUM_HEADS = 8           # Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
FF_DIM = 1024           # ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
NUM_LAYERS = 8          # Ø¹Ø¯Ø¯ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù€ Transformer
BATCH_SIZE = 64         # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙÙØ¹Ø©
EPOCHS = 50             # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±
VOCAB_LIMIT = 50000     # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…ÙØ±Ø¯Ø§Øª
SAMPLE_SIZE = 100000    # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„ØªØ¯Ø±ÙŠØ¨

# 2. ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_and_preprocess_data():
    """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...")
    try:
        dataset = load_dataset("wiki40b", "ar", split="train")
        texts = dataset["text"][:100000]  # 100,000 Ù…Ù‚Ø§Ù„Ø©
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
        return texts
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        print("âš¡ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙŠÙ„Ø©...")
        try:
            dataset = load_dataset("arabic_billion_words", split="train")
            texts = dataset["text"][:100000]
            return texts
        except:
            print("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©")
            return generate_sample_data(10000)

def generate_sample_data(num_samples):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© ØºÙ†ÙŠØ©"""
    base_texts = [
        "Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…ØŒ ÙØ§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù„Ù… Ø¯Ø§Ø¦Ù…Ø§Ù‹",
        "Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„ØµØºØ± ÙƒØ§Ù„Ù†Ù‚Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ø±ØŒ Ù„Ø°Ù„Ùƒ ÙŠØ¬Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø¨ÙƒØ±",
        "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø§Ù„Ø¶Ø§Ø¯ØŒ ÙˆÙ‡ÙŠ Ù…Ù† Ø£Ù‚Ø¯Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø³Ø§Ù…ÙŠØ©",
        "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ØªØ³Ø§Ù‡Ù… ÙÙŠ ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª ÙˆØ§Ø²Ø¯Ù‡Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ",
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ…Ø«Ù„ Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©ØŒ ÙˆØ³ÙŠØºÙŠØ± ÙˆØ¬Ù‡ Ø§Ù„Ø¹Ø§Ù„Ù… ÙÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©",
        "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ø¹ Ø¢ÙØ§Ù‚ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† ÙˆØªØ²ÙŠØ¯ Ù…Ù† Ù…Ø¹Ø±ÙØªÙ‡ ÙˆØªØ·ÙˆØ± Ù…Ù‡Ø§Ø±Ø§ØªÙ‡ Ø§Ù„ÙÙƒØ±ÙŠØ©",
        "Ø§Ù„ØªØ§Ø±ÙŠØ® ÙŠØ¹Ù„Ù…Ù†Ø§ Ø¯Ø±ÙˆØ³Ø§Ù‹ Ù‚ÙŠÙ…Ø© Ø¹Ù† ØµØ¹ÙˆØ¯ ÙˆØ³Ù‚ÙˆØ· Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø£Ù…Ù…",
        "Ø§Ù„ÙÙ„Ø³ÙØ© ØªØ³Ø§Ø¹Ø¯Ù†Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙˆØ¬ÙˆØ¯ÙŠØ© Ø§Ù„ÙƒØ¨Ø±Ù‰",
        "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù‡ÙŠ Ù„ØºØ© Ø§Ù„ÙƒÙˆÙ†ØŒ ÙˆÙ‡ÙŠ Ø£Ø³Ø§Ø³ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ÙŠØ©",
        "Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ø§Ø¨ØªÙƒØ§Ø± Ù‡Ù…Ø§ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ø¨Ø´Ø±ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª"
    ]
    
    # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
    texts = []
    for i in range(num_samples):
        base = base_texts[i % len(base_texts)]
        variations = [
            f"ÙÙŠ Ù…Ø¬Ø§Ù„ {base.split()[0]}ØŒ {base}",
            f"Ù…Ù† Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø£Ù† {base}",
            f"ØªØ¹ØªØ¨Ø± {base}",
            f"Ø¨Ù„Ø§ Ø´ÙƒØŒ {base}",
            f"ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø­Ø¯ÙŠØ«ØŒ {base}"
        ]
        texts.append(variations[i % len(variations)])
    return texts

def preprocess_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    if not isinstance(text, str):
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
    text = re.sub(r'[\u064B-\u065F\u0670]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[^\u0600-\u06FF0-9\sØŒØ›.ØŸ!]', ' ', text)  # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
    text = re.sub(r'\s+', ' ', text).strip()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    
    # ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    if len(text.split()) > 50:
        parts = re.split(r'[.ØŒØ›ØŸ!]', text)
        text = ' '.join(parts[:3])
    
    return text

# 3. ØªØ¬Ù‡ÙŠØ² Tokenizer
def prepare_tokenizer(sentences):
    """Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ù…ØªÙ‚Ø¯Ù…"""
    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ø¬Ø¯ÙŠØ¯...")
    tokenizer = Tokenizer(
        num_words=VOCAB_LIMIT, 
        oov_token="<OOV>",
        filters=''
    )
    tokenizer.fit_on_texts(sentences)
    
    # Ø­ÙØ¸ Tokenizer
    with open(TOKENIZER_FILE, "wb") as f:
        pickle.dump(tokenizer, f)
    
    vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
    print(f"ğŸ”¤ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {vocab_size}")
    return tokenizer, vocab_size

# 4. Ø·Ø¨Ù‚Ø§Øª Transformer Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="gelu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

# 5. Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Transformer Ù…ØªÙ‚Ø¯Ù…
def build_transformer_model(vocab_size, max_len):
    """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Transformer Ù…ØªÙ‚Ø¯Ù…"""
    inputs = Input(shape=(max_len,))
    embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, EMBEDDING_DIM)
    x = embedding_layer(inputs)
    
    # Ø·Ø¨Ù‚Ø§Øª Transformer
    for _ in range(NUM_LAYERS):
        x = TransformerBlock(EMBEDDING_DIM, NUM_HEADS, FF_DIM)(x)
    
    # Ø·Ø¨Ù‚Ø© ØªØ¬Ù…ÙŠØ¹ Ù†Ù‡Ø§Ø¦ÙŠØ©
    x = GlobalAveragePooling1D()(x)
    x = Dropout(0.2)(x)
    
    # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ© Ù…ØªÙ‚Ø¯Ù…Ø©
    x = Dense(FF_DIM, activation="gelu", kernel_regularizer=l2(0.001))(x)
    x = LayerNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(FF_DIM // 2, activation="gelu", kernel_regularizer=l2(0.001))(x)
    x = LayerNormalization()(x)
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    outputs = Dense(vocab_size, activation="softmax")(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    
    # Ø§Ù„Ù…ØªØ±Ø¬Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    optimizer = Adam(
        learning_rate=0.0001,
        beta_1=0.9,
        beta_2=0.98,
        epsilon=1e-9,
        clipnorm=1.0
    )
    
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )
    
    model.summary()
    return model

# 6. Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
class AdvancedDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, sentences, tokenizer, max_seq_len, batch_size, shuffle=True):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.sentences) / self.batch_size))
    
    def __getitem__(self, index):
        batch_sentences = self.sentences[index*self.batch_size:(index+1)*self.batch_size]
        X, y = [], []
        
        for sentence in batch_sentences:
            if not sentence:
                continue
                
            tokens = self.tokenizer.texts_to_sequences([sentence])[0]
            tokens = [t if t < self.vocab_size else 1 for t in tokens]  # 1 = <OOV>
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø©
            for i in range(1, len(tokens)):
                start_idx = max(0, i - self.max_seq_len)
                seq = tokens[start_idx:i+1]
                
                if len(seq) < 2:
                    continue
                    
                padded_seq = pad_sequences([seq], maxlen=self.max_seq_len+1, padding='pre')[0]
                X.append(padded_seq[:-1])
                y.append(padded_seq[-1])
        
        if len(X) == 0:
            return np.zeros((1, self.max_seq_len)), np.zeros((1,))
        
        return np.array(X), np.array(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.sentences)

# 7. Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
def generate_advanced_text(seed_text, next_words, model, tokenizer, max_seq_len, temperature=0.7):
    """ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªØ·ÙˆØ±Ø©"""
    if not seed_text:
        return ""
    
    output = seed_text
    reshaped_output = arabic_reshaper.reshape(seed_text)
    bidi_output = get_display(reshaped_output)
    print(f"ğŸŒ± Ø§Ù„Ø¨Ø°Ø±Ø©: {bidi_output}")
    
    for _ in range(next_words):
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ³Ù„Ø³Ù„
        token_list = tokenizer.texts_to_sequences([output])[0]
        token_list = [t if t < VOCAB_LIMIT else 1 for t in token_list]
        
        if not token_list:
            token_list = [1]  # <OOV>
            
        # Ø§Ù‚ØªØµØ§Ø± Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
        token_list = token_list[-max_seq_len:]
        token_list = pad_sequences([token_list], maxlen=max_seq_len, padding='pre')
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        predictions = model.predict(token_list, verbose=0)[0]
        
        # Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹
        predictions = np.log(predictions) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ø±Ø¬Ø­Ø©
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø£Ùˆ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        output += " " + predicted_word
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø°ÙƒÙŠ Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        if predicted_word in [".", "ØŸ", "!"] and np.random.random() > 0.3:
            break
            
    # ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ù„Ø¹Ø±Ø¶
    reshaped_text = arabic_reshaper.reshape(output)
    bidi_text = get_display(reshaped_text)
    return bidi_text

# 8. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
def main():
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("ğŸ”¥ Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    texts = load_and_preprocess_data()
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
    sentences = []
    for text in texts:
        cleaned = preprocess_text(text)
        if cleaned and len(cleaned.split()) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
            sentences.append(cleaned)
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„
    sentences = sentences[:SAMPLE_SIZE]
    print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(sentences)}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer
    tokenizer, vocab_size = prepare_tokenizer(sentences)
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = build_transformer_model(vocab_size, MAX_SEQUENCE_LEN)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_sents, val_sents = train_test_split(sentences, test_size=0.1, random_state=42)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_generator = AdvancedDataGenerator(train_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE)
    val_generator = AdvancedDataGenerator(val_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE, shuffle=False)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    callbacks = [
        ModelCheckpoint(
            MODEL_FILE, 
            save_best_only=True, 
            monitor='val_accuracy',
            mode='max',
            save_weights_only=False,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=2,
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=LOG_DIR,
            histogram_freq=1,
            profile_batch='10,15'
        )
    ]
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\nğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹)")
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=val_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:")
    test_seeds = [
        "Ø§Ù„Ø¹Ù„Ù… Ù‡Ùˆ Ø£Ø³Ø§Ø³",
        "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©",
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        "Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
        "Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©"
    ]
    
    for seed in test_seeds:
        generated = generate_advanced_text(seed, 20, model, tokenizer, MAX_SEQUENCE_LEN, temperature=0.8)
        reshaped_seed = arabic_reshaper.reshape(seed)
        print(f"\nğŸŒ± Ø§Ù„Ø¨Ø°Ø±Ø©: {get_display(reshaped_seed)}")
        print(f"ğŸ§  Ø¥Ø¨Ø¯Ø§Ø¹ ML-T1:\n{generated}")
        print("â”€" * 70)
    
    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    print("\nğŸ¤– Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¬Ø±Ø¨Ø©! Ø£Ø¯Ø®Ù„ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    while True:
        user_input = input("\n> ").strip()
        if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("âœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        
        user_input = preprocess_text(user_input)
        if not user_input or len(user_input.split()) < 2:
            print("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø¹Ø±Ø¨ÙŠ ØµØ§Ù„Ø­ (ÙƒÙ„Ù…ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)")
            continue
        
        generated = generate_advanced_text(
            user_input, 
            25, 
            model, 
            tokenizer, 
            MAX_SEQUENCE_LEN,
            temperature=0.7
        )
        print(f"\nğŸ§  Ø¥Ø¨Ø¯Ø§Ø¹ ML-T1:\n{generated}")

if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'
    os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    main()
