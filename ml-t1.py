# -*- coding: utf-8 -*-
"""
Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø¥ØµØ¯Ø§Ø± Ù…ØµØ­Ø­
"""

import os
import re
import pickle
import numpy as np
from datasets import load_dataset
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import (
    Embedding, Bidirectional, LSTM, Dense, 
    Dropout, LayerNormalization
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, 
    ReduceLROnPlateau
)
from sklearn.model_selection import train_test_split
import arabic_reshaper
from bidi.algorithm import get_display

# ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
tf.config.optimizer.set_jit(True)
tf.config.run_functions_eagerly(True)

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
MODEL_NAME = "ML-T1-Fixed"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"

# Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø­Ø³Ø¨ Ø·Ù„Ø¨Ùƒ)
MAX_SEQUENCE_LEN = 15  # Ø·ÙˆÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
EMBEDDING_DIM = 128    # Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
LSTM_UNITS = 128       # ÙˆØ­Ø¯Ø§Øª LSTM
DENSE_UNITS = 256      # ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ«ÙŠÙØ©
BATCH_SIZE = 128       # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙÙØ¹Ø©
EPOCHS = 10            # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±
VOCAB_LIMIT = 30000    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…ÙØ±Ø¯Ø§Øª
SAMPLE_SIZE = 1000     # Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„ØªØ¯Ø±ÙŠØ¨

# 2. ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_and_preprocess_data():
    """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø£ØµØºØ±
        texts = [
            "Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…",
            "Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„ØµØºØ± ÙƒØ§Ù„Ù†Ù‚Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ø±",
            "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø§Ù„Ø¶Ø§Ø¯",
            "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ØªØºÙŠØ± Ø§Ù„Ø¹Ø§Ù„Ù… Ø¨Ø³Ø±Ø¹Ø©",
            "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©",
            "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ø¹ Ø¢ÙØ§Ù‚ Ø§Ù„Ø¥Ù†Ø³Ø§Ù†",
            "Ø§Ù„ØªØ§Ø±ÙŠØ® ÙŠØ¹Ù„Ù…Ù†Ø§ Ø¯Ø±ÙˆØ³Ø§Ù‹ Ù‚ÙŠÙ…Ø©",
            "Ø§Ù„ÙÙ„Ø³ÙØ© ØªØ³Ø§Ø¹Ø¯Ù†Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¬ÙˆØ¯",
            "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù„ØºØ© Ø§Ù„ÙƒÙˆÙ†",
            "Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ù‡Ùˆ Ø¬ÙˆÙ‡Ø± Ø§Ù„ØªÙ‚Ø¯Ù…"
        ] * 100  # ØªÙƒØ±Ø§Ø± Ù„Ø¥Ù†Ø´Ø§Ø¡ 1000 Ø¬Ù…Ù„Ø©
        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (1000 Ø¬Ù…Ù„Ø©)")
        return texts
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return []

def preprocess_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    if not isinstance(text, str):
        return ""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø­Ø±ÙˆÙ ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    text = re.sub(r'[\u064B-\u065F]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)  # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
    text = re.sub(r'\s+', ' ', text).strip()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    return text

# 3. ØªØ¬Ù‡ÙŠØ² Tokenizer (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØµØ­Ø­Ø©)
def prepare_tokenizer(sentences):
    """Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø¶Ø¨Ø· Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"""
    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ø¬Ø¯ÙŠØ¯...")
    tokenizer = Tokenizer(num_words=VOCAB_LIMIT, oov_token="<OOV>")
    tokenizer.fit_on_texts(sentences)
    
    # Ø­ÙØ¸ Tokenizer
    with open(TOKENIZER_FILE, "wb") as f:
        pickle.dump(tokenizer, f)
    
    vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
    print(f"ğŸ”¤ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {vocab_size}")
    return tokenizer, vocab_size

# 4. Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØµØ­Ø­Ø©)
class AdvancedDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, sentences, tokenizer, max_seq_len, batch_size, shuffle=True):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.sentences) / self.batch_size))
    
    def __getitem__(self, index):
        batch_sentences = self.sentences[index*self.batch_size:(index+1)*self.batch_size]
        X, y = [], []
        
        for sentence in batch_sentences:
            if not sentence:
                continue
            tokens = self.tokenizer.texts_to_sequences([sentence])[0]
            
            # ØªØ£ÙƒÙŠØ¯ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ­ÙŠØ­
            tokens = [t if t < self.vocab_size else 1 for t in tokens]  # 1 = <OOV>
            
            for i in range(1, len(tokens)):
                start_idx = max(0, i - self.max_seq_len + 1)
                seq = tokens[start_idx:i+1]
                if len(seq) < 2:
                    continue
                padded_seq = pad_sequences([seq], maxlen=self.max_seq_len, padding='pre')[0]
                X.append(padded_seq[:-1])
                y.append(padded_seq[-1])
        
        if len(X) == 0:
            return np.zeros((1, self.max_seq_len-1)), np.zeros((1,))
        
        return np.array(X), np.array(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.sentences)

# 5. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØµØ­Ø­Ø©)
def build_advanced_model(vocab_size, max_seq_len):
    model = Sequential(name=MODEL_NAME)
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ø¹ ØªØ£ÙƒÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
    model.add(Embedding(
        input_dim=vocab_size,
        output_dim=EMBEDDING_DIM,
        mask_zero=True
    ))
    
    model.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)))
    model.add(LayerNormalization())
    model.add(Dropout(0.3))
    
    model.add(Bidirectional(LSTM(LSTM_UNITS // 2)))
    model.add(LayerNormalization())
    model.add(Dropout(0.3))
    
    model.add(Dense(DENSE_UNITS, activation='relu'))
    model.add(LayerNormalization())
    model.add(Dropout(0.4))
    
    model.add(Dense(DENSE_UNITS // 2, activation='relu'))
    model.add(LayerNormalization())
    
    model.add(Dense(vocab_size, activation='softmax'))
    
    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)
    
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    
    model.build((None, max_seq_len-1))
    model.summary()
    return model

# 6. Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
def generate_deep_text(seed_text, next_words, model, tokenizer, max_seq_len, temperature=0.7):
    if not seed_text:
        return ""
    
    output = seed_text
    
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([output])[0]
        
        # ØªØ£ÙƒÙŠØ¯ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ­ÙŠØ­
        token_list = [t if t < model.input_shape[-1] else 1 for t in tokenizer.texts_to_sequences([output])[0]]
        
        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')
        
        predictions = model.predict(token_list, verbose=0)[0]
        predictions = np.log(predictions) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        output += " " + predicted_word
        
        if predicted_word in [".", "ØŸ", "!"]:
            break
            
    reshaped_text = arabic_reshaper.reshape(output)
    return get_display(reshaped_text)

# 7. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØµØ­Ø­Ø©)
def main():
    print("âš ï¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ CPU (ÙŠÙˆØµÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ù…Ø«Ù„)")
    
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    texts = load_and_preprocess_data()
    if not texts:
        print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return
    
    sentences = []
    for text in texts:
        cleaned = preprocess_text(text)
        if cleaned:
            sentences.append(cleaned)
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ‚Ø· Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„
    sentences = sentences[:SAMPLE_SIZE]
    print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(sentences)}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer Ø¬Ø¯ÙŠØ¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹
    tokenizer, vocab_size = prepare_tokenizer(sentences)
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = build_advanced_model(vocab_size, MAX_SEQUENCE_LEN)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_sents, val_sents = train_test_split(sentences, test_size=0.1, random_state=42)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_generator = AdvancedDataGenerator(train_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE)
    val_generator = AdvancedDataGenerator(val_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE, shuffle=False)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    callbacks = [
        ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
        EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-6, verbose=1)
    ]
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\nğŸ”¥ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=val_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:")
    test_seeds = ["Ø§Ù„Ø¹Ù„Ù… Ù‡Ùˆ", "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"]
    for seed in test_seeds:
        generated = generate_deep_text(seed, 8, model, tokenizer, MAX_SEQUENCE_LEN)
        print(f"\nØ§Ù„Ø¨Ø°Ø±Ø©: {seed}\nØ§Ù„Ø¥ÙƒÙ…Ø§Ù„: {generated}")
    
    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    print("\nğŸ¤– Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¬Ø±Ø¨Ø©! Ø£Ø¯Ø®Ù„ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    while True:
        user_input = input("> ").strip()
        if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            break
        generated = generate_deep_text(user_input, 10, model, tokenizer, MAX_SEQUENCE_LEN)
        print(f"Ø§Ù„Ø¥ÙƒÙ…Ø§Ù„: {generated}\n")

if __name__ == "__main__":
    main()