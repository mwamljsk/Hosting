# -*- coding: utf-8 -*-
"""
Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡
ØªÙ… ØªØµÙ…ÙŠÙ…Ù‡ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ù…ÙŠÙ‚
"""

import os
import re
import pickle
import numpy as np
import multiprocessing
from datasets import load_dataset
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import (
    Embedding, Bidirectional, LSTM, Dense, 
    Dropout, LayerNormalization
)
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, 
    ReduceLROnPlateau, TensorBoard
)
from sklearn.model_selection import train_test_split
import arabic_reshaper
from bidi.algorithm import get_display

# ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
tf.config.optimizer.set_jit(True)
tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
MODEL_NAME = "ML-T1-Advanced"
MODEL_FILE = f"{MODEL_NAME}.h5"
TOKENIZER_FILE = f"{MODEL_NAME}_tokenizer.pkl"
LOG_DIR = f"{MODEL_NAME}_logs"

# Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
MAX_SEQUENCE_LEN = 15  # Ø·ÙˆÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
EMBEDDING_DIM = 256    # Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
LSTM_UNITS = 512       # ÙˆØ­Ø¯Ø§Øª LSTM
DENSE_UNITS = 1024     # ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ«ÙŠÙØ©
BATCH_SIZE = 2048      # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙÙØ¹Ø©
EPOCHS = 50            # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±
VOCAB_LIMIT = 50000    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…ÙØ±Ø¯Ø§Øª

# 2. ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_and_preprocess_data():
    """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Wiki40B (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)...")
    dataset = load_dataset("wiki40b", "ar", split="train")
    texts = dataset["text"][:100000]  # 100,000 Ù…Ù‚Ø§Ù„Ø©
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
    print("ğŸ§¹ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ (Ù…ØªÙˆØ§Ø²ÙŠ)...")
    with multiprocessing.Pool() as pool:
        cleaned_texts = pool.map(preprocess_text, texts)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø¬Ù…Ù„
    print("ğŸ”¢ Ø¬Ø§Ø±ÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„...")
    sentences = []
    for text in cleaned_texts:
        parts = re.split(r"[\.ØŸ!\n]", text)
        for part in parts:
            if part.strip():
                sentences.append(part.strip())
    
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² {len(sentences)} Ø¬Ù…Ù„Ø©")
    return sentences

def preprocess_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø­Ø±ÙˆÙ ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    text = re.sub(r'[\u064B-\u065F]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)  # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
    text = re.sub(r'\s+', ' ', text).strip()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    return text

# 3. ØªØ¬Ù‡ÙŠØ² Tokenizer
def prepare_tokenizer(sentences):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„ Tokenizer"""
    if os.path.exists(TOKENIZER_FILE):
        with open(TOKENIZER_FILE, "rb") as f:
            tokenizer = pickle.load(f)
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Tokenizer Ù…Ù†: {TOKENIZER_FILE}")
    else:
        tokenizer = Tokenizer(num_words=VOCAB_LIMIT, oov_token="<OOV>")
        tokenizer.fit_on_texts(sentences)
        
        # Ø­ÙØ¸ Tokenizer
        with open(TOKENIZER_FILE, "wb") as f:
            pickle.dump(tokenizer, f)
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­ÙØ¸ Tokenizer ÙÙŠ: {TOKENIZER_FILE}")
    
    vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
    print(f"ğŸ”¤ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {vocab_size}")
    return tokenizer, vocab_size

# 4. Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
class AdvancedDataGenerator(tf.keras.utils.Sequence):
    """Ù…ÙˆÙ„Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"""
    def __init__(self, sentences, tokenizer, max_seq_len, batch_size, shuffle=True):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_LIMIT)
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.sentences) / self.batch_size))
    
    def __getitem__(self, index):
        batch_sentences = self.sentences[index*self.batch_size:(index+1)*self.batch_size]
        X, y = [], []
        
        for sentence in batch_sentences:
            tokens = self.tokenizer.texts_to_sequences([sentence])[0]
            for i in range(1, len(tokens)):
                start_idx = max(0, i - self.max_seq_len + 1)
                seq = tokens[start_idx:i+1]
                padded_seq = pad_sequences([seq], maxlen=self.max_seq_len, padding='pre')[0]
                X.append(padded_seq[:-1])
                y.append(padded_seq[-1])
        
        return np.array(X), to_categorical(y, num_classes=self.vocab_size)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.sentences)

# 5. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
def build_advanced_model(vocab_size, max_seq_len):
    """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    model = Sequential(name="ML-T1-Advanced")
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    model.add(Embedding(
        input_dim=vocab_size,
        output_dim=EMBEDDING_DIM,
        input_length=max_seq_len - 1,
        mask_zero=True
    ))
    
    # Ø·Ø¨Ù‚Ø§Øª LSTM Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
    model.add(Bidirectional(LSTM(
        LSTM_UNITS, 
        return_sequences=True,
        kernel_regularizer=tf.keras.regularizers.l2(0.001)
    ))
    model.add(LayerNormalization())
    model.add(Dropout(0.3))
    
    model.add(Bidirectional(LSTM(
        LSTM_UNITS // 2,
        kernel_regularizer=tf.keras.regularizers.l2(0.001))
    ))
    model.add(LayerNormalization())
    model.add(Dropout(0.3))
    
    # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ©
    model.add(Dense(DENSE_UNITS, activation='relu'))
    model.add(LayerNormalization())
    model.add(Dropout(0.4))
    
    model.add(Dense(DENSE_UNITS // 2, activation='relu'))
    model.add(LayerNormalization())
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    model.add(Dense(vocab_size, activation='softmax'))
    
    # Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù…Ø¹ Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    optimizer = AdamW(
        learning_rate=0.001,
        weight_decay=0.0001,
        clipnorm=1.0
    )
    
    model.compile(
        loss='categorical_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    
    model.summary()
    return model

# 6. Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
def generate_deep_text(seed_text, next_words, model, tokenizer, max_seq_len, temperature=0.7):
    """ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø¹Ù…ÙŠÙ‚ Ù…Ø¹ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹"""
    output = seed_text
    reshaped_output = arabic_reshaper.reshape(seed_text)
    bidi_output = get_display(reshaped_output)
    print(f"ğŸŒ± Ø§Ù„Ø¨Ø°Ø±Ø©: {bidi_output}")
    
    for i in range(next_words):
        token_list = tokenizer.texts_to_sequences([output])[0]
        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ ØªØ­ÙƒÙ… ÙÙŠ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        predictions = model.predict(token_list, verbose=0)[0]
        predictions = np.log(predictions) / max(temperature, 0.1)
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds)
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ø¹ Ù…Ø±Ø¬Ø­Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        predicted_idx = np.random.choice(len(probs), p=probs)
        predicted_word = tokenizer.index_word.get(predicted_idx, "")
        
        # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        if not predicted_word or predicted_word == "<OOV>":
            break
            
        output += " " + predicted_word
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        if predicted_word in [".", "ØŸ", "!"] and i > next_words//2:
            break
            
    # ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¹Ø±Ø¶
    reshaped_text = arabic_reshaper.reshape(output)
    bidi_text = get_display(reshaped_text)
    return bidi_text

# 7. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
def main():
    # ØªÙ‡ÙŠØ¦Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    strategy = tf.distribute.MirroredStrategy() if tf.config.list_physical_devices('GPU') else None
    
    if strategy:
        print(f"ğŸš€ ØªÙ… Ø§Ù„ÙƒØ´Ù Ø¹Ù† {strategy.num_replicas_in_sync} GPUs")
        global BATCH_SIZE
        BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync
        with strategy.scope():
            sentences = load_and_preprocess_data()
            tokenizer, vocab_size = prepare_tokenizer(sentences)
            model = build_advanced_model(vocab_size, MAX_SEQUENCE_LEN)
    else:
        print("âš ï¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ CPU (ÙŠÙˆØµÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ù…Ø«Ù„)")
        sentences = load_and_preprocess_data()
        tokenizer, vocab_size = prepare_tokenizer(sentences)
        model = build_advanced_model(vocab_size, MAX_SEQUENCE_LEN)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_sents, val_sents = train_test_split(
        sentences, test_size=0.1, random_state=42
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_generator = AdvancedDataGenerator(
        train_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE
    )
    
    val_generator = AdvancedDataGenerator(
        val_sents, tokenizer, MAX_SEQUENCE_LEN, BATCH_SIZE, shuffle=False
    )
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    callbacks = [
        ModelCheckpoint(
            filepath=MODEL_FILE,
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            save_weights_only=False,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=LOG_DIR,
            histogram_freq=1,
            profile_batch='10,15'
        )
    ]
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\nğŸ”¥ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ ML-T1 Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø³Ø§Ø¹Ø§Øª Ø¥Ù„Ù‰ Ø£ÙŠØ§Ù…)")
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=val_generator,
        callbacks=callbacks,
        workers=multiprocessing.cpu_count(),
        use_multiprocessing=True
    )
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\nğŸ¯ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø¬Ø§Ø±ÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    test_seeds = [
        "Ø§Ù„Ø¹Ù„Ù… Ù‡Ùˆ Ø£Ø³Ø§Ø³",
        "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ØªØ³Ø§Ù‡Ù… ÙÙŠ",
        "Ø§Ù„ÙÙ„Ø³ÙØ© ØªØ¨Ø­Ø« Ø¹Ù†",
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø³ÙˆÙ"
    ]
    
    for seed in test_seeds:
        generated = generate_deep_text(
            seed, 20, model, tokenizer, MAX_SEQUENCE_LEN, temperature=0.8
        )
        print(f"\nğŸŒ± Ø§Ù„Ø¨Ø°Ø±Ø©: {get_display(arabic_reshaper.reshape(seed))}")
        print(f"ğŸ§  Ø¥Ø¨Ø¯Ø§Ø¹ ML-T1:\n{generated}")
        print("â”€" * 50)
    
    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    print("\nğŸ¤– Ù†Ù…ÙˆØ°Ø¬ ML-T1 Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¬Ø±Ø¨Ø©! Ø£Ø¯Ø®Ù„ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø®Ø±ÙˆØ¬.")
    while True:
        seed = input("\n> ").strip()
        if seed.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("âœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        
        seed = preprocess_text(seed)
        if not seed:
            print("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø¹Ø±Ø¨ÙŠ ØµØ§Ù„Ø­.")
            continue
        
        generated = generate_deep_text(
            seed, 15, model, tokenizer, MAX_SEQUENCE_LEN, temperature=0.7
        )
        print(f"\nğŸ§  Ø¥Ø¨Ø¯Ø§Ø¹ ML-T1:\n{generated}")

if __name__ == "__main__":
    main()
